{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Section_2_BERT.ipynb","provenance":[{"file_id":"1N_eeSYN-wK4lKpex3MnDXbistuFKrBVh","timestamp":1626189963736},{"file_id":"1zuNa6dVM24Ytpod7H8ay5wOux8KV17UL","timestamp":1590776159819}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"18a52e5f67084fb08654a2c42f14ae2a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_a97bbfbd68ac43658b4415dff470cb3f","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_fcbeb35b997b4d9a8dc2e971913067a1","IPY_MODEL_5292e86b634240c0867d5172ade99b43"]}},"a97bbfbd68ac43658b4415dff470cb3f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"fcbeb35b997b4d9a8dc2e971913067a1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_a9a85db167da4c7eb77639403228bfad","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":213450,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":213450,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8bbdccb50b0e4dc880a0eef1f86d7048"}},"5292e86b634240c0867d5172ade99b43":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_3a5133ca6f834a3592e23f55234b6816","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 213k/213k [00:02&lt;00:00, 82.3kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d697f9798eae4d9d869a51e175c863d8"}},"a9a85db167da4c7eb77639403228bfad":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"8bbdccb50b0e4dc880a0eef1f86d7048":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3a5133ca6f834a3592e23f55234b6816":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"d697f9798eae4d9d869a51e175c863d8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1adb16cbb3ad42cab644f97de494fb51":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_131e4597c0e54019949f25903d5f1668","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_35c5db684e9f450a91ab82addc227109","IPY_MODEL_012c9cfc5dfd47c680946c3c2b0c0a4b"]}},"131e4597c0e54019949f25903d5f1668":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"35c5db684e9f450a91ab82addc227109":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_63e36f1a0e3340b29bb3f9f4fe925f3a","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":29,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":29,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_662ea2d5ed804a56b92071cf288208af"}},"012c9cfc5dfd47c680946c3c2b0c0a4b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_ece656322a0e403ba1a14164e2f10c0e","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 29.0/29.0 [00:00&lt;00:00, 29.4B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_618266a5a5c648b18a448d06cf5c9361"}},"63e36f1a0e3340b29bb3f9f4fe925f3a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"662ea2d5ed804a56b92071cf288208af":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ece656322a0e403ba1a14164e2f10c0e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"618266a5a5c648b18a448d06cf5c9361":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4c13cc88e9d449b6b64f648e07241a46":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_8c0c27d7b3174264a15f52663b178568","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_8bfebea8c5ab4184b8a00c4ce8c88c62","IPY_MODEL_3118191e32a440e889de22e63ec42f5a"]}},"8c0c27d7b3174264a15f52663b178568":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8bfebea8c5ab4184b8a00c4ce8c88c62":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_22b339156f964f20bb151220ec6358a1","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":435797,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":435797,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_90e43fc92bfa4f29beb6ac440b5f5b7c"}},"3118191e32a440e889de22e63ec42f5a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_2374a1666902463fb37c42f7753f647c","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 436k/436k [00:00&lt;00:00, 2.08MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ef8bf66029be4158b5fe574f1ff1cc51"}},"22b339156f964f20bb151220ec6358a1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"90e43fc92bfa4f29beb6ac440b5f5b7c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2374a1666902463fb37c42f7753f647c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ef8bf66029be4158b5fe574f1ff1cc51":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ff6aa92906174a4b9dd2239fdf008266":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_b695a6932ec548f5a2d7954b226da550","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_d71963e4b648477ea71c1e2317cc244e","IPY_MODEL_aefcb9d0a2da44fd87af25ced3b32545"]}},"b695a6932ec548f5a2d7954b226da550":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d71963e4b648477ea71c1e2317cc244e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_5eb61da2cef4470a93a4eb27655a3406","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":570,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":570,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d57157f0b8da495b90b355706a133e3a"}},"aefcb9d0a2da44fd87af25ced3b32545":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_b4de63134c5844aeafcec3f76cfa895d","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 570/570 [00:00&lt;00:00, 1.07kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a9945b0decb44fc294e2ee3441359937"}},"5eb61da2cef4470a93a4eb27655a3406":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"d57157f0b8da495b90b355706a133e3a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b4de63134c5844aeafcec3f76cfa895d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"a9945b0decb44fc294e2ee3441359937":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"10cf32243caa439fb1092632651e2283":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_661447f76ab84b8ca68115c0b42ed96b","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_35eef9751781451fa995682cbecbca37","IPY_MODEL_88e332ba1f31461cbd9e1173d81be2ff"]}},"661447f76ab84b8ca68115c0b42ed96b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"35eef9751781451fa995682cbecbca37":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_0c6b88d086694fe080fe4ef68ae53ff7","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":526681800,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":526681800,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_36e7fbad87ab4f45b64bef1b584a3623"}},"88e332ba1f31461cbd9e1173d81be2ff":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_df2c1766790c4b289113abfcd1cdadf1","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 527M/527M [00:22&lt;00:00, 23.7MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6ff4142e58894c0c9198cd3f756abff5"}},"0c6b88d086694fe080fe4ef68ae53ff7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"36e7fbad87ab4f45b64bef1b584a3623":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"df2c1766790c4b289113abfcd1cdadf1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"6ff4142e58894c0c9198cd3f756abff5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"m0ygrZd3dmu5"},"source":["# Classifying Voice Commands\n","\n","For voice commands, Siri needs to be able to figure out *what* the speaker wants, and then *how* to accomplish that request. \n","\n","<img src=\"https://www.cheatsheet.com/wp-content/uploads/2016/01/Siri-in-iOS-9-640x305.png\" width=400>\n","\n","The focus of this notebook is predicting the 'what', in other words, the **intent** of a voice command.\n","\n","Remember that this is part (a) of our two-part goal.\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"dlE9nndIa-4-"},"source":["In this notebook we'll be:\n","*   Preprocessing the Dataset for the ML models\n","*   Implementing a Pretrained BERT model"]},{"cell_type":"markdown","metadata":{"id":"irPDgkzsdguU"},"source":["## Sentence Level Classification\n","\n","\n","\n","\n","Here is where our friend BERT comes in to help us. As a reminder, **BERT** stands for **B**idirectional **E**ncoder **R**epresentations from **T**ransformers, and is a novel model architecture that uses both the left and right *context* words in a sequence to generate good embeddings for words. \n","\n","We can make use of existing Pytorch APIs for BERT that will allow us to convert our voice command sequence data into the format that the BERT model expects, and then use *pretrained* BERT for our classification task.\n","\n","<img src=\"https://pytorch.org/tutorials/_images/bert.png\" height=\"300\">\n","\n","Note that there are many available pretrained BERT models with varying attributes such as: the language of the corpus for training, how many layers and attention heads are used, if the corpus for pretraining contains uncased or cased words, and so on. We will use the cased BERT base, which is the architecture described in the slides (12 layers - 6 layers within the encoder and 6 layers within the decoder - and 12 attention heads), and is trained on cased English text.\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Nmzc3C9XkIKL"},"source":["**IMPORTANT**: Since the BERT model we will be using in this notebook is so large, we need to do one step before continuing. Please go to the 'Runtime' tab, and click on 'Change Runtime Type'; then select **GPU** under the dropdown for Hardware accelerator."]},{"cell_type":"code","metadata":{"id":"PjzYKqcweS2F","cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626192777297,"user_tz":420,"elapsed":10856,"user":{"displayName":"Ravi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6SX3WlQTXibD6hgiep50H9CpJepKU0fmumT7JAs0=s64","userId":"16311629485750149971"}},"outputId":"f9608d8d-9bff-427e-d212-b33f407a59e1"},"source":["#@title Run this code to get started\n","%tensorflow_version 2.x\n","%pip install -q transformers\n","\n","from urllib.request import urlretrieve\n","from pathlib import Path\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","\n","# SNIPS_DATA_BASE_URL = (\n","#     \"https://github.com/ogrisel/slot_filling_and_intent_detection_of_SLU/blob/\"\n","#     \"master/data/snips/\"\n","# )\n","# for filename in [\"train\", \"valid\", \"test\", \"vocab.intent\", \"vocab.slot\"]:\n","#     path = Path(filename)\n","#     if not path.exists():\n","#       print(f\"Downloading {filename}...\")\n","#       urlretrieve(SNIPS_DATA_BASE_URL + filename + \"?raw=true\", path)\n","\n","!wget 'https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20Siri%20(Bert)%20Voice%20Commands/train'\n","!wget 'https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20Siri%20(Bert)%20Voice%20Commands/valid'\n","!wget 'https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20Siri%20(Bert)%20Voice%20Commands/test'\n","!wget 'https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20Siri%20(Bert)%20Voice%20Commands/vocab.intent'\n","!wget 'https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20Siri%20(Bert)%20Voice%20Commands/vocab.slot'\n","\n","\n","\n","def parse_line(line):\n","    data, intent_label = line.split(\" <=> \")\n","    items = data.split()\n","    words = [item.rsplit(\":\", 1)[0]for item in items]\n","    word_labels = [item.rsplit(\":\", 1)[1]for item in items]\n","    return {\n","        \"intent_label\": intent_label, ### YOUR CODE HERE ###, \n","        \"words\": \" \".join(words),\n","        \"word_labels\": \" \".join(word_labels), ### YOUR CODE HERE ###,\n","        \"length\": len(words), ### YOUR CODE HERE ###,\n","    }\n","\n","\n","train_lines = Path(\"train\").read_text().strip().splitlines()\n","valid_lines = Path(\"valid\").read_text().strip().splitlines()\n","test_lines = Path(\"test\").read_text().strip().splitlines()\n","\n","df_train = pd.DataFrame([parse_line(line) for line in train_lines])\n","df_valid = pd.DataFrame([parse_line(line) for line in valid_lines])\n","df_test = pd.DataFrame([parse_line(line) for line in test_lines])"],"execution_count":2,"outputs":[{"output_type":"stream","text":["\u001b[K     |████████████████████████████████| 2.5MB 12.7MB/s \n","\u001b[K     |████████████████████████████████| 3.3MB 24.9MB/s \n","\u001b[K     |████████████████████████████████| 901kB 37.0MB/s \n","\u001b[?25h--2021-07-13 16:12:54--  https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20Siri%20(Bert)%20Voice%20Commands/train\n","Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.140.128, 108.177.15.128, 173.194.76.128, ...\n","Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.140.128|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1793794 (1.7M) [application/octet-stream]\n","Saving to: ‘train’\n","\n","train               100%[===================>]   1.71M  --.-KB/s    in 0.02s   \n","\n","2021-07-13 16:12:55 (77.8 MB/s) - ‘train’ saved [1793794/1793794]\n","\n","--2021-07-13 16:12:55--  https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20Siri%20(Bert)%20Voice%20Commands/valid\n","Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.140.128, 108.177.15.128, 173.194.76.128, ...\n","Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.140.128|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 98609 (96K) [application/octet-stream]\n","Saving to: ‘valid’\n","\n","valid               100%[===================>]  96.30K  --.-KB/s    in 0.001s  \n","\n","2021-07-13 16:12:56 (86.8 MB/s) - ‘valid’ saved [98609/98609]\n","\n","--2021-07-13 16:12:56--  https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20Siri%20(Bert)%20Voice%20Commands/test\n","Resolving storage.googleapis.com (storage.googleapis.com)... 142.251.5.128, 74.125.206.128, 64.233.167.128, ...\n","Connecting to storage.googleapis.com (storage.googleapis.com)|142.251.5.128|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 96761 (94K) [application/octet-stream]\n","Saving to: ‘test’\n","\n","test                100%[===================>]  94.49K  --.-KB/s    in 0.001s  \n","\n","2021-07-13 16:12:56 (74.1 MB/s) - ‘test’ saved [96761/96761]\n","\n","--2021-07-13 16:12:56--  https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20Siri%20(Bert)%20Voice%20Commands/vocab.intent\n","Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.140.128, 108.177.15.128, 173.194.76.128, ...\n","Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.140.128|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 99 [application/octet-stream]\n","Saving to: ‘vocab.intent’\n","\n","vocab.intent        100%[===================>]      99  --.-KB/s    in 0s      \n","\n","2021-07-13 16:12:56 (15.2 MB/s) - ‘vocab.intent’ saved [99/99]\n","\n","--2021-07-13 16:12:56--  https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20Siri%20(Bert)%20Voice%20Commands/vocab.slot\n","Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.206.128, 64.233.184.128, 64.233.167.128, ...\n","Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.206.128|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 994 [application/octet-stream]\n","Saving to: ‘vocab.slot’\n","\n","vocab.slot          100%[===================>]     994  --.-KB/s    in 0s      \n","\n","2021-07-13 16:12:57 (15.0 MB/s) - ‘vocab.slot’ saved [994/994]\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"PAc67kxQftEV"},"source":["Remember that `df_train`, `df_valid`, and `df_test` are the Pandas Dataframes that store our training, validation, and test datapoints."]},{"cell_type":"markdown","metadata":{"id":"D0ZWheGxd5XT"},"source":["### Step 1: Tokenize our Data\n","\n","BERT expects input sequences to be broken down into individual *tokens*; a `[CLS]` token marks the beginning and a `[SEP]` marks the end of a given sequence (as seen in the red input sentences in the image above).\n","\n","The `[CLS]` token is used by BERT for the pre-training task for sequence classification.\n","\n","The `[SEP]` token is a separator for the pre-training task that classifies if a pair of sentences are consecutive in a corpus or not, i.e. next sentence prediction (NSP)."]},{"cell_type":"code","metadata":{"id":"2VLvVuJUchlR","colab":{"base_uri":"https://localhost:8080/","height":164,"referenced_widgets":["18a52e5f67084fb08654a2c42f14ae2a","a97bbfbd68ac43658b4415dff470cb3f","fcbeb35b997b4d9a8dc2e971913067a1","5292e86b634240c0867d5172ade99b43","a9a85db167da4c7eb77639403228bfad","8bbdccb50b0e4dc880a0eef1f86d7048","3a5133ca6f834a3592e23f55234b6816","d697f9798eae4d9d869a51e175c863d8","1adb16cbb3ad42cab644f97de494fb51","131e4597c0e54019949f25903d5f1668","35c5db684e9f450a91ab82addc227109","012c9cfc5dfd47c680946c3c2b0c0a4b","63e36f1a0e3340b29bb3f9f4fe925f3a","662ea2d5ed804a56b92071cf288208af","ece656322a0e403ba1a14164e2f10c0e","618266a5a5c648b18a448d06cf5c9361","4c13cc88e9d449b6b64f648e07241a46","8c0c27d7b3174264a15f52663b178568","8bfebea8c5ab4184b8a00c4ce8c88c62","3118191e32a440e889de22e63ec42f5a","22b339156f964f20bb151220ec6358a1","90e43fc92bfa4f29beb6ac440b5f5b7c","2374a1666902463fb37c42f7753f647c","ef8bf66029be4158b5fe574f1ff1cc51"]},"executionInfo":{"status":"ok","timestamp":1626192789361,"user_tz":420,"elapsed":7535,"user":{"displayName":"Ravi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6SX3WlQTXibD6hgiep50H9CpJepKU0fmumT7JAs0=s64","userId":"16311629485750149971"}},"outputId":"61e80ac3-bf19-4e13-d035-8e2e8cac3f6b"},"source":["# Import the BERT tokenizer and utilize the BERT base pretrained model\n","from transformers import BertTokenizer\n","\n","model_name = \"bert-base-cased\"\n","tokenizer = BertTokenizer.from_pretrained(model_name)"],"execution_count":3,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"18a52e5f67084fb08654a2c42f14ae2a","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=213450.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1adb16cbb3ad42cab644f97de494fb51","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=29.0, style=ProgressStyle(description_w…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4c13cc88e9d449b6b64f648e07241a46","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=435797.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CPdGS07td863"},"source":["How many unique tokens in this BERT model are there?"]},{"cell_type":"code","metadata":{"id":"NDKUw6svd9Im","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626192794361,"user_tz":420,"elapsed":521,"user":{"displayName":"Ravi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6SX3WlQTXibD6hgiep50H9CpJepKU0fmumT7JAs0=s64","userId":"16311629485750149971"}},"outputId":"a71b663d-f924-41b2-9863-d246766df393"},"source":["tokenizer.vocab_size"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["28996"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"C7gd0Pw8eA7B"},"source":["Wow - there's roughly 30k tokens that BERT knows! <img src=\"https://i7.pngguru.com/preview/625/201/218/emoji-iphone-computer-icons-clip-art-emoji-thumbnail.jpg\" width=20>\n","\n","Let's see exactly how the BERT tokenizer works on one training example."]},{"cell_type":"code","metadata":{"id":"wf4zLAdaeBR1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626192807026,"user_tz":420,"elapsed":533,"user":{"displayName":"Ravi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6SX3WlQTXibD6hgiep50H9CpJepKU0fmumT7JAs0=s64","userId":"16311629485750149971"}},"outputId":"daa7d495-1af1-462f-df5c-02b6eed3c458"},"source":["# Print the first training data sequence\n","first_sentence = df_train.iloc[0][\"words\"]\n","print(first_sentence)\n","print(\"This sentence contains {} words.\".format(len(first_sentence.split(\" \"))))"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Add Don and Sherri to my Meditate to Sounds of Nature playlist\n","This sentence contains 12 words.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kusXEf2JeFDg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626192809974,"user_tz":420,"elapsed":264,"user":{"displayName":"Ravi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6SX3WlQTXibD6hgiep50H9CpJepKU0fmumT7JAs0=s64","userId":"16311629485750149971"}},"outputId":"c376623b-045b-467b-9b40-5cf07d956dfa"},"source":["tokens = tokenizer.tokenize(first_sentence)\n","for t in tokens:\n","  print(t)\n","print(\"\\nThis sentence turns into {} tokens.\".format(len(tokens)))"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Ad\n","##d\n","Don\n","and\n","She\n","##rri\n","to\n","my\n","Me\n","##dit\n","##ate\n","to\n","Sounds\n","of\n","Nature\n","play\n","##list\n","\n","This sentence turns into 17 tokens.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rUUM0qHugJcw"},"source":["Hmmm, why are there 17 *tokens* now but only 12 *words* within the original sequence.\n","\n","It's because BERT uses something called subword tokenization!\n","\n","**Subword tokenization** may break some words into smaller units and doesn't always use a 1:1 mapping of words to tokens. Because of subword tokenization within BERT, the length of tokenized sentences is likely to be larger than the original number of words in the sentence.\n","\n","**Discuss:**\n","\n","Why is it interesting and useful to use subword tokenization for generic language models such as BERT?"]},{"cell_type":"markdown","metadata":{"id":"gJGWM5J-gOZV"},"source":["### Step 2: Encoding Tokens\n","\n","Each token is mapped to a unique integer id that makes it fast to lookup the correct column in the input layer token embedding (seen in yellow below).\n","\n","<img src=\"https://miro.medium.com/max/1496/1*fywysLzEbXxEqkPeIUWaJw.png\" width=\"500\">"]},{"cell_type":"code","metadata":{"id":"_3yBoMYqgG0R","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626193973614,"user_tz":420,"elapsed":292,"user":{"displayName":"Ravi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6SX3WlQTXibD6hgiep50H9CpJepKU0fmumT7JAs0=s64","userId":"16311629485750149971"}},"outputId":"e385391f-0ca5-4dbd-eab6-9bb5ea2ff0b5"},"source":["# See the first 10 (token,id) pairs within BERT.\n","bert_vocab_items = list(tokenizer.vocab.items())\n","bert_vocab_items[:10]"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('[PAD]', 0),\n"," ('[unused1]', 1),\n"," ('[unused2]', 2),\n"," ('[unused3]', 3),\n"," ('[unused4]', 4),\n"," ('[unused5]', 5),\n"," ('[unused6]', 6),\n"," ('[unused7]', 7),\n"," ('[unused8]', 8),\n"," ('[unused9]', 9)]"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"TOnuTWJvgQ7P","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626193976912,"user_tz":420,"elapsed":618,"user":{"displayName":"Ravi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6SX3WlQTXibD6hgiep50H9CpJepKU0fmumT7JAs0=s64","userId":"16311629485750149971"}},"outputId":"c8bd0f0d-d33a-4a29-e686-e3c42aec5eb2"},"source":["# See more (token,id) examples within BERT's vocab.\n","bert_vocab_items[1100:1110]"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('－', 1100),\n"," ('／', 1101),\n"," ('：', 1102),\n"," ('the', 1103),\n"," ('of', 1104),\n"," ('and', 1105),\n"," ('to', 1106),\n"," ('in', 1107),\n"," ('was', 1108),\n"," ('The', 1109)]"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"ZjTk9qkyvViH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626193980181,"user_tz":420,"elapsed":247,"user":{"displayName":"Ravi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6SX3WlQTXibD6hgiep50H9CpJepKU0fmumT7JAs0=s64","userId":"16311629485750149971"}},"outputId":"6b3a9cd0-cf9c-41d2-d661-6c133c62ffcd"},"source":["# Why are these tokens interesting?\n","bert_vocab_items[100:104]"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('[UNK]', 100), ('[CLS]', 101), ('[SEP]', 102), ('[MASK]', 103)]"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"1RJypthuvYns"},"source":["(The `[UNK]` token is used to replace tokens that BERT doesn't recognize. If there is no way to split a word into subtokens, the whole word becomes `[UNK]`.)\n","\n","`[CLS]` marks the beginning of a sentence, and `[SEP]` marks the end. \n","\n","`[MASK]` is used by BERT in the pre-training task called MLM (Masked Language Model) where it learns to predict the original word if it's masked out in a sentence."]},{"cell_type":"markdown","metadata":{"id":"qbCq5LpQgVkF"},"source":["Let's analyze the first training sequence again."]},{"cell_type":"code","metadata":{"id":"_pQUHmuDgSyn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626195140535,"user_tz":420,"elapsed":254,"user":{"displayName":"Ravi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6SX3WlQTXibD6hgiep50H9CpJepKU0fmumT7JAs0=s64","userId":"16311629485750149971"}},"outputId":"4081a160-6f42-47ee-fa89-779f52d79fa6"},"source":["# See the integer indexes for the token embeddings of the first sentence.\n","idxs = tokenizer.encode(first_sentence)\n","print(idxs)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["[101, 24930, 1181, 1790, 1105, 1153, 14791, 1106, 1139, 2508, 17903, 2193, 1106, 10560, 1104, 7009, 1505, 7276, 102]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Ht-HqEmMgXZy","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1626195144677,"user_tz":420,"elapsed":2018,"user":{"displayName":"Ravi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6SX3WlQTXibD6hgiep50H9CpJepKU0fmumT7JAs0=s64","userId":"16311629485750149971"}},"outputId":"b004f040-bc49-4aa8-891e-cd7b8d9e085a"},"source":["# Decode the encoding, which should produce the original sequence.\n","tokenizer.decode(idxs)"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'[CLS] Add Don and Sherri to my Meditate to Sounds of Nature playlist [SEP]'"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"FzWrP7kYgdVo"},"source":["### Step 3: Padding Tokenized Sequences"]},{"cell_type":"markdown","metadata":{"id":"0XlouEr3ggRN"},"source":["#### Exercise 1\n","\n","In the Training Data Analysis section of the 1st notebook, we created a histogram according to the lengths of training sequences. Let's take a look at the histogram of the **tokenized** sequences now."]},{"cell_type":"code","metadata":{"id":"uUSq9VhTggmm","colab":{"base_uri":"https://localhost:8080/","height":295},"executionInfo":{"status":"ok","timestamp":1626195725681,"user_tz":420,"elapsed":4704,"user":{"displayName":"Ravi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6SX3WlQTXibD6hgiep50H9CpJepKU0fmumT7JAs0=s64","userId":"16311629485750149971"}},"outputId":"ba735cb5-aa28-422f-bda1-959c9289d816"},"source":["# Print histogram of training tokenized sequence lengths.\n","# Hint: use tokenizer.encode on every sequence within df_train\n","# & get the lengths; then use plt.hist()\n","\n","train_sequence_lengths = [len(tokenizer.encode(text)) for text in df_train.words]\n","\n","plt.hist(train_sequence_lengths, bins=20)\n","plt.xlabel(\"Number of tokens per sequence\")\n","plt.ylabel(\"Count of sequences\")\n","plt.title(\"Token Sequence Length Histogram\")\n","plt.show()"],"execution_count":16,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"KN_8RLj2hRiD"},"source":["What is the maximum length of any given tokenized training sequence?\n","\n","*Remember that the maximum of the original voice command lengths was 35 words per sequence.*"]},{"cell_type":"code","metadata":{"id":"6Os61ui7hPxH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626195906500,"user_tz":420,"elapsed":286,"user":{"displayName":"Ravi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6SX3WlQTXibD6hgiep50H9CpJepKU0fmumT7JAs0=s64","userId":"16311629485750149971"}},"outputId":"f96ec279-bc40-4fd9-f572-4abd2bac6edd"},"source":["max_token_len = max(train_sequence_lengths)\n","print(\"Maximum tokenized sequence length: {} tokens per sequence\".format(max_token_len))"],"execution_count":17,"outputs":[{"output_type":"stream","text":["Maximum tokenized sequence length: 43 tokens per sequence\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"SyCTT3WPhpec"},"source":["**Transfer learning**, i.e. using *pretrained* BERT applied to our new dataset and sentiment classification task, requires that the sequences are **padded**, which means they all have the same length.\n","\n","The above histogram shows that after tokenization, 43 tokens is long enough to represent all the voice commands in the training set."]},{"cell_type":"markdown","metadata":{"id":"6LoeTBKEhrev"},"source":["### Preprocessing Full Dataset\n","\n","<img src=\"https://miro.medium.com/max/2152/1*vNGH3DTu83pZr3gVHsz7yg.png\" width=500>\n"]},{"cell_type":"markdown","metadata":{"id":"ZJGK5kL9xHtH"},"source":["Let's now encode the full train, validation, and test sets with the BERT tokenizer to get padded integer numpy arrays."]},{"cell_type":"code","metadata":{"id":"nFDJmLtghpuX","executionInfo":{"status":"ok","timestamp":1626195917373,"user_tz":420,"elapsed":253,"user":{"displayName":"Ravi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6SX3WlQTXibD6hgiep50H9CpJepKU0fmumT7JAs0=s64","userId":"16311629485750149971"}}},"source":["def encode_dataset(text_sequences):\n","    # Create token_ids array (initialized to all zeros), where \n","    # rows are a sequence and columns are encoding ids\n","    # of each token in given sequence.\n","    token_ids = np.zeros(shape=(len(text_sequences), max_token_len),\n","                         dtype=np.int32)\n","    \n","    for i, text_sequence in enumerate(text_sequences):\n","        encoded = tokenizer.encode(text_sequence)\n","        token_ids[i, 0:len(encoded)] = encoded\n","\n","    attention_masks = (token_ids != 0).astype(np.int32)\n","    return {\"input_ids\": token_ids, \"attention_masks\": attention_masks}"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"fJ2GF91YhvIB","executionInfo":{"status":"ok","timestamp":1626195926246,"user_tz":420,"elapsed":4563,"user":{"displayName":"Ravi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6SX3WlQTXibD6hgiep50H9CpJepKU0fmumT7JAs0=s64","userId":"16311629485750149971"}}},"source":["encoded_train = encode_dataset(df_train[\"words\"])"],"execution_count":19,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zlfF7RWKh5rV"},"source":["Let's see what the output of the function `encode_dataset` is on the training data:"]},{"cell_type":"code","metadata":{"id":"qA4uyGmrhwqq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626195929788,"user_tz":420,"elapsed":525,"user":{"displayName":"Ravi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6SX3WlQTXibD6hgiep50H9CpJepKU0fmumT7JAs0=s64","userId":"16311629485750149971"}},"outputId":"bccec42c-b30f-433c-f07b-9cfced9fb8d2"},"source":["encoded_train[\"input_ids\"]"],"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[  101, 24930,  1181, ...,     0,     0,     0],\n","       [  101,  1508,  1244, ...,     0,     0,     0],\n","       [  101,  5194,  1103, ...,     0,     0,     0],\n","       ...,\n","       [  101, 27640,  1116, ...,     0,     0,     0],\n","       [  101,  5979,  6608, ...,     0,     0,     0],\n","       [  101,  1327,  2523, ...,     0,     0,     0]], dtype=int32)"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"ExBeEvwQh7h-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626195932790,"user_tz":420,"elapsed":777,"user":{"displayName":"Ravi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6SX3WlQTXibD6hgiep50H9CpJepKU0fmumT7JAs0=s64","userId":"16311629485750149971"}},"outputId":"2cfd0d36-44de-4ab9-9bf0-51a7205211b1"},"source":["encoded_train[\"attention_masks\"]"],"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[1, 1, 1, ..., 0, 0, 0],\n","       [1, 1, 1, ..., 0, 0, 0],\n","       [1, 1, 1, ..., 0, 0, 0],\n","       ...,\n","       [1, 1, 1, ..., 0, 0, 0],\n","       [1, 1, 1, ..., 0, 0, 0],\n","       [1, 1, 1, ..., 0, 0, 0]], dtype=int32)"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"markdown","metadata":{"id":"pD13PjPih-2A"},"source":["**Discuss**:\n","\n","What is the purpose of the attention_masks array?\n","(Hint: Think about what each row and column corresponds to).\n"]},{"cell_type":"markdown","metadata":{"id":"KSEQpuOSiAtp"},"source":["#### Exercise 2\n","\n","Let's also create the encoding id and attention mask arrays for the validation and test sets:"]},{"cell_type":"code","metadata":{"id":"KsHrCy2miCag","executionInfo":{"status":"ok","timestamp":1626196327030,"user_tz":420,"elapsed":1449,"user":{"displayName":"Ravi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6SX3WlQTXibD6hgiep50H9CpJepKU0fmumT7JAs0=s64","userId":"16311629485750149971"}}},"source":["# Create the necessary np arrays using the encode_dataset function\n","# and store your results in encoded_valid, and encoded_test.\n","\n","### YOUR CODE HERE ###\n","encoded_valid = encode_dataset(df_valid.words)\n","encoded_test = encode_dataset(df_test.words)"],"execution_count":24,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kPj_8jJPiKIf"},"source":["#### Encoding the Sequence Classification Targets\n","\n","Aside from encodings of the tokenized word sequences, we also need to get the encodings of the target intent classification labels."]},{"cell_type":"code","metadata":{"id":"t-eOu3PbiKWx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626196362742,"user_tz":420,"elapsed":259,"user":{"displayName":"Ravi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6SX3WlQTXibD6hgiep50H9CpJepKU0fmumT7JAs0=s64","userId":"16311629485750149971"}},"outputId":"29b18b23-dcce-4a69-fa53-91fbeb0de45b"},"source":["# Build a map from target intent label to a unique id.\n","intent_names = Path(\"vocab.intent\").read_text().split()\n","intent_map = dict((label, idx) for idx, label in enumerate(intent_names))\n","intent_map"],"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'AddToPlaylist': 0,\n"," 'BookRestaurant': 1,\n"," 'GetWeather': 2,\n"," 'PlayMusic': 3,\n"," 'RateBook': 4,\n"," 'SearchCreativeWork': 5,\n"," 'SearchScreeningEvent': 6}"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"code","metadata":{"id":"v5bTeYoRiNVQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626196364849,"user_tz":420,"elapsed":8,"user":{"displayName":"Ravi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6SX3WlQTXibD6hgiep50H9CpJepKU0fmumT7JAs0=s64","userId":"16311629485750149971"}},"outputId":"c0672c0f-fbe2-4271-c6e8-c1a452dac76e"},"source":["# Convert list of target labels into their corresponding unique id.\n","intent_train = df_train[\"intent_label\"].map(intent_map).values\n","intent_train"],"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0, 0, 0, ..., 6, 6, 6])"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"code","metadata":{"id":"qnzGflSGiPC0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626196366755,"user_tz":420,"elapsed":492,"user":{"displayName":"Ravi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6SX3WlQTXibD6hgiep50H9CpJepKU0fmumT7JAs0=s64","userId":"16311629485750149971"}},"outputId":"0fb8bfcc-d380-4739-e6cf-d468af22e06b"},"source":["# Sanity check: the number of sentences that belongs to each class\n","for i in range(7):\n","  print(len(intent_train[intent_train == i]))"],"execution_count":27,"outputs":[{"output_type":"stream","text":["1842\n","1873\n","1900\n","1900\n","1856\n","1854\n","1859\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nbY64mzriRb6"},"source":["<img src=\"https://drive.google.com/uc?id=1i9qommblASpRkuJWCtyRo2E1xYd7IgfA\" height=\"200\">\n","\n","Great! This is the same as we found within the first notebook (screenshot of that output above)."]},{"cell_type":"markdown","metadata":{"id":"w6WNpz_1iZ1m"},"source":["##### Exercise 3\n","\n","Lastly, we need to encode the intent labels for the validation and training sets as well."]},{"cell_type":"code","metadata":{"id":"YjouAdSiiRtp","executionInfo":{"status":"ok","timestamp":1626196447326,"user_tz":420,"elapsed":510,"user":{"displayName":"Ravi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6SX3WlQTXibD6hgiep50H9CpJepKU0fmumT7JAs0=s64","userId":"16311629485750149971"}}},"source":["# Create two variables named intent_valid and intent_test\n","# Hint: look at how intent_train is defined a few cells above. \n","\n","### YOUR CODE HERE ###\n","intent_valid = df_valid.intent_label.map(intent_map).values\n","intent_test = df_test.intent_label.map(intent_map).values"],"execution_count":28,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LBNRFoRIihgh"},"source":["### Using a Pretrained BERT model\n","\n","Yay - we have all the pre-processing steps done! Now it's time to explore pretrained BERT. <img src=\"https://i.pinimg.com/originals/58/e9/78/58e97802c32da4c19d8429823c3201c2.jpg\" width=25>"]},{"cell_type":"code","metadata":{"id":"QdsPIw6Uih_d","colab":{"base_uri":"https://localhost:8080/","height":390,"referenced_widgets":["ff6aa92906174a4b9dd2239fdf008266","b695a6932ec548f5a2d7954b226da550","d71963e4b648477ea71c1e2317cc244e","aefcb9d0a2da44fd87af25ced3b32545","5eb61da2cef4470a93a4eb27655a3406","d57157f0b8da495b90b355706a133e3a","b4de63134c5844aeafcec3f76cfa895d","a9945b0decb44fc294e2ee3441359937","10cf32243caa439fb1092632651e2283","661447f76ab84b8ca68115c0b42ed96b","35eef9751781451fa995682cbecbca37","88e332ba1f31461cbd9e1173d81be2ff","0c6b88d086694fe080fe4ef68ae53ff7","36e7fbad87ab4f45b64bef1b584a3623","df2c1766790c4b289113abfcd1cdadf1","6ff4142e58894c0c9198cd3f756abff5"]},"executionInfo":{"status":"ok","timestamp":1626196477496,"user_tz":420,"elapsed":27139,"user":{"displayName":"Ravi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6SX3WlQTXibD6hgiep50H9CpJepKU0fmumT7JAs0=s64","userId":"16311629485750149971"}},"outputId":"b1ed14d7-5a99-4231-9115-98318b372fc1"},"source":["from transformers import TFBertModel\n","\n","# recall that we earlier defined model_name to be \"bert-base-cased\"\n","base_bert_model = TFBertModel.from_pretrained(model_name)\n","base_bert_model.summary()"],"execution_count":29,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ff6aa92906174a4b9dd2239fdf008266","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=570.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"10cf32243caa439fb1092632651e2283","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=526681800.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n","- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"],"name":"stderr"},{"output_type":"stream","text":["Model: \"tf_bert_model\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","bert (TFBertMainLayer)       multiple                  108310272 \n","=================================================================\n","Total params: 108,310,272\n","Trainable params: 108,310,272\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vumnko-qiy-F"},"source":["Let's see what the pretrained BERT model outputs when we feed it our encoded validation set:"]},{"cell_type":"code","metadata":{"id":"jZEJmpSPjQyT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626196477760,"user_tz":420,"elapsed":281,"user":{"displayName":"Ravi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6SX3WlQTXibD6hgiep50H9CpJepKU0fmumT7JAs0=s64","userId":"16311629485750149971"}},"outputId":"4192e88f-78b2-4b64-c936-f8d38107149d"},"source":["# Note: This cell might take a bit of time to run.\n","outputs = base_bert_model(encoded_valid)\n","len(outputs)"],"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"code","metadata":{"id":"FZVA5Cm7jTgU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626196481060,"user_tz":420,"elapsed":241,"user":{"displayName":"Ravi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6SX3WlQTXibD6hgiep50H9CpJepKU0fmumT7JAs0=s64","userId":"16311629485750149971"}},"outputId":"33bd201c-8b5e-4827-de0b-87d5fb4ab9b1"},"source":["print(encoded_valid.values())"],"execution_count":31,"outputs":[{"output_type":"stream","text":["dict_values([array([[  101,   142, 13894, ...,     0,     0,     0],\n","       [  101,  2825,   179, ...,     0,     0,     0],\n","       [  101, 24930,  1181, ...,     0,     0,     0],\n","       ...,\n","       [  101,  1525,   170, ...,     0,     0,     0],\n","       [  101,  4630,  1143, ...,     0,     0,     0],\n","       [  101,  1327,  1159, ...,     0,     0,     0]], dtype=int32), array([[1, 1, 1, ..., 0, 0, 0],\n","       [1, 1, 1, ..., 0, 0, 0],\n","       [1, 1, 1, ..., 0, 0, 0],\n","       ...,\n","       [1, 1, 1, ..., 0, 0, 0],\n","       [1, 1, 1, ..., 0, 0, 0],\n","       [1, 1, 1, ..., 0, 0, 0]], dtype=int32)])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"fQlDWVZdnYZZ"},"source":["#### Outputs of BERT \n","\n","We can see that there are two outputs from the BERT model.\n","\n","\n","1. The **first** output of the BERT model is a tensor with shape: `(batch_size, seq_len, output_dim)` which computes **features for each token in the input sequence**"]},{"cell_type":"code","metadata":{"id":"M5DDe2kanYy4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626196486970,"user_tz":420,"elapsed":307,"user":{"displayName":"Ravi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6SX3WlQTXibD6hgiep50H9CpJepKU0fmumT7JAs0=s64","userId":"16311629485750149971"}},"outputId":"70f6031c-33fa-47c4-aa19-d495ca68263a"},"source":["token_features = outputs[0]\n","token_features.shape"],"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TensorShape([700, 43, 768])"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"markdown","metadata":{"id":"WED3ODi-n5Zx"},"source":["Note that `seq_len` is `max_token_len` (ie. 43)."]},{"cell_type":"markdown","metadata":{"id":"Ik14njDsn7Uf"},"source":["\n","2. \n","\n","The **second** output of the BERT model is a tensor with shape: `(batch_size, output_dim)` which is the feature vector of `[CLS]`. This vector is typically used as a **pooled representation for the sequence as a whole**. We can use this as the features of our intent classifier"]},{"cell_type":"code","metadata":{"id":"kxpxokzpnaVv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626196545891,"user_tz":420,"elapsed":240,"user":{"displayName":"Ravi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6SX3WlQTXibD6hgiep50H9CpJepKU0fmumT7JAs0=s64","userId":"16311629485750149971"}},"outputId":"92f83a90-7d84-4372-b654-a6a3b3df0d59"},"source":["sentence_representation = outputs[1]\n","sentence_representation.shape"],"execution_count":33,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TensorShape([700, 768])"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"markdown","metadata":{"id":"EDNxGswlga65"},"source":["Remember that our goal is to use BERT to compute some **representation** of a single voice command at a time. \n","\n","We have two options to obtain this sentence-level representation, which is used as the input for the final sequence classification layer:\n","\n","1. We can reuse the representation of the `[CLS]` token.\n","\n","or\n","\n","2. We can pool the representations (encodings) of all the tokens within the sequence (i.e. global average)."]},{"cell_type":"markdown","metadata":{"id":"S6Fh7rYXzhaK"},"source":["#### Pooled Representation"]},{"cell_type":"markdown","metadata":{"id":"t3u4v-8In_KG"},"source":["The pooled output comes from the application of a small multi-layer-perception (MLP) layer called the \"Pooler\" which is applied to the output representation of the special `[CLS]` token.\n","\n","<img src=\"https://miro.medium.com/max/3446/1*-IPQlOd46dlsutIbUq1Zcw.png\" width=300>"]},{"cell_type":"code","metadata":{"id":"qhZE-GFLn_7w","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626196569039,"user_tz":420,"elapsed":517,"user":{"displayName":"Ravi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6SX3WlQTXibD6hgiep50H9CpJepKU0fmumT7JAs0=s64","userId":"16311629485750149971"}},"outputId":"5ff04660-9b2b-4da6-8568-23368d6de978"},"source":["base_bert_model.bert.pooler.dense"],"execution_count":34,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.layers.core.Dense at 0x7f1ecd270c10>"]},"metadata":{"tags":[]},"execution_count":34}]},{"cell_type":"code","metadata":{"id":"lxopUQmroBXy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626196571437,"user_tz":420,"elapsed":9,"user":{"displayName":"Ravi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6SX3WlQTXibD6hgiep50H9CpJepKU0fmumT7JAs0=s64","userId":"16311629485750149971"}},"outputId":"26912b15-6535-40b1-bcdc-014cb959b8ec"},"source":["# Extract the features across all batches for the 0th ([CLS]) token \n","first_token_states = token_features[:, 0]\n","pooled_outputs = base_bert_model.bert.pooler.dense(first_token_states)\n","pooled_outputs.shape"],"execution_count":35,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TensorShape([700, 768])"]},"metadata":{"tags":[]},"execution_count":35}]},{"cell_type":"markdown","metadata":{"id":"VROGcIWBoFTL"},"source":["Now let's double-check that taking this route of extracting all of the token features for `[CLS]` and passing it through the \"Pooler\" results in close to the same output as using simply the `sentence_representation`.\n","\n","Take a look at the [documentation](https://numpy.org/doc/1.18/reference/generated/numpy.allclose.html) for the `allclose` function used below if you're interested."]},{"cell_type":"code","metadata":{"id":"L0cjv9IMoCyY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626196658656,"user_tz":420,"elapsed":516,"user":{"displayName":"Ravi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6SX3WlQTXibD6hgiep50H9CpJepKU0fmumT7JAs0=s64","userId":"16311629485750149971"}},"outputId":"bc1312fc-07f1-4f87-9258-c7252db34e19"},"source":["np.allclose(pooled_outputs, sentence_representation)"],"execution_count":36,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":36}]},{"cell_type":"markdown","metadata":{"id":"FDQc9ZhgoJkW"},"source":["### Exercise 4\n","\n","Time to make classifications!\n","\n","Let's finish filling out the following code template to build and train a **sequence classification model** to predict the **intent class** of voice commands.\n","\n","Remember: we're currently only considering the pooled sentence features and ignoring the token-wise features for now.\n","\n","Documentation for a [Dropout](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout) and [Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) layer."]},{"cell_type":"code","metadata":{"id":"Iw18sQdQoJzf"},"source":["import tensorflow as tf\n","from transformers import TFBertModel\n","from tensorflow.keras.layers import Dropout, Dense, GlobalAveragePooling1D\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.losses import SparseCategoricalCrossentropy\n","from tensorflow.keras.metrics import SparseCategoricalAccuracy\n","\n","\n","class IntentClassificationModel(tf.keras.Model):\n","\n","    def __init__(self, intent_num_labels=None,\n","                 dropout_prob=0.1):\n","        super().__init__(name=\"intent_classifier\")\n","\n","        # Load the pretrained BERT model in the constructor\n","        self.bert = base_bert_model\n","\n","        # TODO: Specify the dropout\n","        self.dropout = Dropout(dropout_prob)  ### YOUR CODE HERE ###\n","\n","        # TODO: define a Dense classification layer which will compute\n","        # the intent for each sequence in a batch. The number of \n","        # output classes is given by the intent_num_labels parameter.\n","        # Use the default linear activation (no softmax) to compute logits.\n","        # The softmax normalization will be computed in the loss function\n","        # instead of the model.\n","        self.intent_classifier = Dense(intent_num_labels)  ### YOUR CODE HERE ###\n","\n","    def call(self, inputs, **kwargs):\n","        # Use the pretrained model to extract features from our encoded inputs:\n","        tokens_output, pooled_output = self.bert(inputs, **kwargs, return_dict=False)\n","\n","        # The second output of the main BERT layer has shape \n","        # (batch_size, output_dim) and gives the pooled representation\n","        # for the full sequence (from the hidden state corresponding to [CLS]).\n","        pooled_output = self.dropout(pooled_output, \\\n","                                     training=kwargs.get(\"training\", False))\n","        \n","        # TODO: use classifier layer to compute logits from pooled features.\n","        intent_logits = self.intent_classifier(pooled_output)  ### YOUR CODE HERE ###\n","\n","        return intent_logits\n","\n","# TODO: create an instantiation of this class and pass in the correct\n","# parameter for intent_num_labels.\n","intent_model = None  ### YOUR CODE HERE ###\n","\n","intent_model.compile(optimizer=Adam(learning_rate=3e-5, epsilon=1e-08),\n","                     loss=SparseCategoricalCrossentropy(from_logits=True),\n","                     metrics=[SparseCategoricalAccuracy('accuracy')], run_eagerly=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FjHVILozos-K"},"source":["# Train the model.\n","# Note: this cell will take a bit of time to execute.\n","history = intent_model.fit(encoded_train, intent_train, epochs=1, batch_size=32, \\\n","                           validation_data=(encoded_valid, intent_valid))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vr_1B13Xo8st"},"source":["#### Softmax\n","\n","Note that this classification model outputs **logits** and we need the **probabilities** of each of the 7 classes being the true intent label.\n","\n","Recall that the softmax function turns an input vector of K real numbers into a normalized probability distribution consisting of K probabilities, proportional to the exponentials of the input numbers.\n","\n","<img src=\"https://miro.medium.com/max/1812/1*670CdxchunD-yAuUWdI7Bw.png\" width=400>\n","\n","For us, the final **softmax** normalization layer is included in the loss function instead of the model directly. \n","\n","That's why we need to configure the loss function `SparseCategoricalCrossentropy(from_logits=True)` accordingly above."]},{"cell_type":"markdown","metadata":{"id":"H8VnzpgMo_Jf"},"source":["### Classification\n","\n","The last step to making predictions is writing a `classify` function that will use the `tokenizer` (from Step 1: Tokenize our Data) and the `intent_model` (from Exercise 4).\n","\n","The main point is choosing which of the 7 classes has the highest probability after the softmax."]},{"cell_type":"code","metadata":{"id":"sE9q5SL4o8-w"},"source":["# Classification takes a voice command as input, as well as\n","# the 7 possible classes (intent_names), and uses the BERT\n","# tokenizer and the intent_model.\n","def classify(text, intent_names):\n","    inputs = tf.constant(tokenizer.encode(text))[None, :]  # batch_size = 1\n","    class_id = intent_model(inputs).numpy().argmax(axis=1)[0]\n","    return intent_names[class_id]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pzNCHZycpC0J"},"source":["Let's see how our classify function works on some examples!"]},{"cell_type":"code","metadata":{"id":"vh5Yj-8VpJag"},"source":["classify(\"Book a table for two at La Tour d'Argent for Friday night.\", intent_names)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IVO_ZZehpGQo"},"source":["classify(\"I would like to listen to Anima by Thom Yorke.\", intent_names)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vuR4AA4upKUU"},"source":["classify(\"Will it snow tomorrow in Saclay?\", intent_names)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b3JO9E3spNI4"},"source":["classify(\"Where can I see to the last Star Wars near Odéon tonight?\", intent_names)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fqmRD5-epOxl"},"source":["Yay! These examples are all classified correctly. It's also impressive that we can achieve 98% accuracy on the validation set after training for only 1 epoch."]},{"cell_type":"markdown","metadata":{"id":"rfyKGhq5pRjQ"},"source":["#### Error Analysis\n","\n","Let's do a brute force search through the testing set to qualitatively analyze the types of mistakes that our `intent_model` *is* making though.\n"]},{"cell_type":"code","metadata":{"id":"RMiCU58HpPRa"},"source":["# Loop through each row in the pandas DF for the training set,\n","# and print the sequence, predicted label, and correct label\n","# if our classifier failed to predict the correct intent label.\n","# Limit the output to only the first 20 errors.\n","\n","# Shuffle the dataframe (to look at diverse samples):\n","df_test = df_test.sample(frac=1).reset_index(drop=True)\n","\n","error_count = 0\n","for i in range(len(df_test)):\n","  words = \" \".join(df_test[i:i+1][\"words\"].to_string().split(\" \")[4:])\n","  predicted = classify(words, intent_names)\n","  true = df_test[i:i+1][\"intent_label\"].to_string().split(\" \")[-1]\n","\n","  if predicted != true:\n","    error_count += 1\n","    print(\"Incorrectly predicted {} but the correct label is {} for the sequence {}.\" \\\n","          .format(predicted, true, words))\n","  if error_count == 20:\n","    break"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ABkUb5k6pYUr"},"source":["**Discuss**: Why do you think our classifier made these mistakes, and what could possibly improve the predictions?"]}]}